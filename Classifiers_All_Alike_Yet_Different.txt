http://www.pymvpa.org/tutorial_classifiers.html

Classifiers â€“ All Alike, Yet Different


>>> from mvpa2.tutorial_suite import * >>> ds = get_haxby2001_data()


>>> clf = kNN(k=1, dfx=one_minus_correlation, voting='majority')


>>> clf.train(ds)


A trained classifier can subsequently be used to perform classification of unlabeled samples.


>>> predictions = clf.predict(ds.samples) >>> np.mean(predictions == ds.sa.targets) 1.0


>>> print ds.sa.runtype ['even' 'even' 'even' 'even' 'even' 'even' 'even' 'even' 'odd' 'odd' 'odd'  'odd' 'odd' 'odd' 'odd' 'odd']


>>> ds_split1 = ds[ds.sa.runtype == 'odd'] >>> len(ds_split1) 8 >>> ds_split2 = ds[ds.sa.runtype == 'even'] >>> len(ds_split2) 8


mismatch error


>>> clf.set_postproc(BinaryFxNode(mean_mismatch_error, 'targets')) >>> clf.train(ds_split2) >>> err = clf(ds_split1) >>> print np.asscalar(err) 0.125


estimate the transfer error after swapping the roles


>>> clf.train(ds_split1) >>> err = clf(ds_split2) >>> print np.asscalar(err) 0.0


>> Cross-validation


>>> # disable post-processing again >>> clf.set_postproc(None) >>> # dataset generator >>> hpart = HalfPartitioner(attr='runtype') >>> # complete cross-validation facility >>> cv = CrossValidation(clf, hpart)


it will return the results of all cross-validation folds


>>> cv_results = cv(ds) >>> np.mean(cv_results) 0.0625


>>> len(cv_results) 2 >>> cv_results.samples array([[ 0.   ],        [ 0.125]])


>> Any classifier, really


>>> clf = kNN(k=1, dfx=one_minus_correlation, voting='majority') >>> cvte = CrossValidation(clf, HalfPartitioner(attr='runtype')) >>> cv_results = cvte(ds) >>> np.mean(cv_results)


>>> clf = LinearCSVMC() >>> cvte = CrossValidation(clf, HalfPartitioner(attr='runtype')) >>> cv_results = cvte(ds) >>> np.mean(cv_results) 0.1875


Weâ€™ll get back to the classifiers shortly


custom ones


>>> cvte = CrossValidation(clf, HalfPartitioner(attr='runtype'), ...                        errorfx=lambda p, t: np.mean(p == t)) >>> cv_results = cvte(ds) >>> np.mean(cv_results) 0.8125



